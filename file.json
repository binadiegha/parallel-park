This study employs a comparative experimental design to systematically evaluate the performance of model-free and model-based reinforcement learning (RL) algorithms in the context of the autonomous parallel parking task. A simulation-based approach was adopted primarily due to considerations of safety, cost, and feasibility, as highlighted in the preceding section. Real-world experimentation with autonomous vehicles involves substantial risk of collision, hardware damage, and financial overheads, making a simulated testbed the most pragmatic option at this stage of investigation. 

For the implementation, the study utilised the Highway-Env toolkit, a two-dimensional (2D) simulation environment that provides lightweight yet highly configurable traffic scenarios for reinforcement learning research. Compared to more complex three-dimensional (3D) alternatives such as CARLA, Highway-Env was selected because it offers significantly reduced computational overheads and shorter training times, thereby allowing more rapid prototyping and experimentation. Furthermore, the integration of CARLA into this project presented considerable technical bottlenecks—such as dependency resolution, GPU configuration, and extended setup time—that were incompatible with the project's strict timeframe, especially given delays in acquiring necessary computational resources. 

A key challenge of the chosen approach was that parallel parking functionality is not natively available in Highway-Env. To address this, a custom parking environment was developed by extending the framework's existing API. Highway-Env, built on top of the Gymnasium toolkit, offers modular design and extensibility, enabling the construction of new tasks with relatively minimal overhead. The custom environment inherited from the AbstractEnv class, which provided access to core attributes, rendering utilities, and configuration management. This inheritance allowed us to adapt the base environment's functionality to define parking slots, vehicle spawn points, and goal conditions specific to parallel parking. 

Environment Setup 
To ensure reproducibility and to minimise dependency conflicts, the experimental setup involved the creation of isolated Conda virtual environments for different algorithmic configurations. For instance, separate environments were established for model-free (DQN/PPO) and model-based (DreamerV3) implementations, named ppo-env and dreamer-env respectively. The parallel parking environment was then formally registered under Highway-Env, enabling seamless integration into the training scripts. This modular separation facilitated not only cleaner dependency management but also simplified debugging and version control across the two algorithmic pipelines. 

Overall, this design provided a balance between computational feasibility and experimental rigour, ensuring that the study could focus on evaluating the relative strengths of model-based versus model-free RL approaches within a controlled yet realistic simulation